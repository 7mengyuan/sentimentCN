Group = "group" , width = 800, height = 800,
linkColour = "#666", file = "networks.html",
opacity = 0.9, zoom = TRUE)
d3ForceNetwork(Links = net, Nodes = node_group,
Source = "source", Target = "target",
Value = "value", NodeID = "name",
Group = "group" , width = 800, height = 800,
linkColour = "red", file = "networks.html",
opacity = 0.9, zoom = TRUE)
d3ForceNetwork(Links = net, Nodes = node_group,
Source = "source", Target = "target",
Value = "value",
NodeID = "name", Group = "group" ,
width = 800, height = 800,
linkColour = "red", textColour = "orange",
file = "networks.html",
opacity = 0.9, zoom = TRUE)
d3ForceNetwork(Links = net, Nodes = node_group,
Source = "source", Target = "target",
Value = "value",
NodeID = "name", Group = "group" ,
width = 800, height = 800,
linkColour = "red", textColour = "orange",
file = "networks.html",
opacity = 0.9)
d3ForceNetwork(Links = net, Nodes = node_group,
Source = "source", Target = "target",
Value = "value",
NodeID = "name", Group = "group" ,
width = 800, height = 800,
linkColour = "red",
file = "networks.html",
opacity = 0.9)
?d3ForceNetwork
d3ForceNetwork(Links = net, Nodes = node_group,
Source = "source", Target = "target", Value = "value",
NodeID = "name", Group = "group" ,
fontsize = 8, linkDistance = 50
width = 800, height = 800,
linkColour = "red",
file = "networks.html",
opacity = 0.9)
d3ForceNetwork(Links = net, Nodes = node_group,
Source = "source", Target = "target", Value = "value",
NodeID = "name", Group = "group" ,
fontsize = 8, linkDistance = 50,
width = 800, height = 800,
linkColour = "red",
file = "networks.html",
opacity = 0.9)
d3ForceNetwork(Links = net, Nodes = node_group,
Source = "source", Target = "target", Value = "value",
NodeID = "name", Group = "group" ,
fontsize = 8, linkDistance = 250,
width = 800, height = 800,
linkColour = "red",
file = "networks.html",
opacity = 0.9)
d3SimpleNetwork(net, width = 1800, height = 1800, file = "networks1.html",
textColour = "orange", linkColour = "red",
opacity = 0.9)
install.packges("knitr")
install.packages("knitr")
# basss diffusion model
# basss diffusion model
require(knitr)
install.packages("knitr")
detect_poisson_process(y)
source('D:/possion_process.R')
detect_poisson_process = function(y){
x=table(y)
freq = as.vector(x)
count = as.numeric(names(x))
nfreq = rep(0, max(count) + 1)
nfreq[count + 1] = freq
freq = nfreq
count = 0:max(count)
n = length(count)
df = df - 1
p.hat = diff(c(0, ppois(count[-n], lambda = mean(y)),  1))
expected = sum(freq) * p.hat
# plot
plot(count,freq,type="s",ylim=c(0,max(freq,expected)), main="Poison density and histogram")
lines(count,expected, col="red")
# calculate chisquare
chisq = sum((freq-expected)^2/expected)
df = n-1-1
pvalue=pchisq(chisq,df) ; pvalue
if (pvalue > 0.05){print "It is a poisson process"}
else(print "It is not a poisson process")
return(pvalue)
}
x=table(y)
freq = as.vector(x)
count = as.numeric(names(x))
nfreq = rep(0, max(count) + 1)
nfreq[count + 1] = freq
freq = nfreq
count = 0:max(count)
n = length(count)
df = df - 1
p.hat = diff(c(0, ppois(count[-n], lambda = mean(y)),  1))
expected = sum(freq) * p.hat
# plot
plot(count,freq,type="s",ylim=c(0,max(freq,expected)), main="Poison density and histogram")
lines(count,expected, col="red")
# calculate chisquare
chisq = sum((freq-expected)^2/expected)
df = n-1-1
pvalue=pchisq(chisq,df) ; pvalue
if (pvalue > 0.05){print "It is a poisson process"}
else{print "It is not a poisson process"}
pvalue
set.seed(1);y=rpois(100,5)
x=table(y)
freq = as.vector(x)
count = as.numeric(names(x))
nfreq = rep(0, max(count) + 1)
nfreq[count + 1] = freq
freq = nfreq
count = 0:max(count)
n = length(count)
df = df - 1
p.hat = diff(c(0, ppois(count[-n], lambda = mean(y)),  1))
expected = sum(freq) * p.hat
# plot
plot(count,freq,type="s",ylim=c(0,max(freq,expected)), main="Poison density and histogram")
lines(count,expected, col="red")
# calculate chisquare
chisq = sum((freq-expected)^2/expected)
df = n-1-1
pvalue=pchisq(chisq,df) ; pvalue
if (pvalue > 0.05){print "It is a poisson process"}
else {print "It is not a poisson process"}
print "a"
?print
if (pvalue > 0.05){
print ("It is a poisson process")
} else {print ("It is not a poisson process")}
source('D:/possion_process.R')
detect_poisson_process = function(y){
x=table(y)
freq = as.vector(x)
count = as.numeric(names(x))
nfreq = rep(0, max(count) + 1)
nfreq[count + 1] = freq
freq = nfreq
count = 0:max(count)
n = length(count)
df = df - 1
p.hat = diff(c(0, ppois(count[-n], lambda = mean(y)),  1))
expected = sum(freq) * p.hat
# plot
plot(count,freq,type="s",ylim=c(0,max(freq,expected)), main="Poison density and histogram")
lines(count,expected, col="red")
# calculate chisquare
chisq = sum((freq-expected)^2/expected)
df = n-1-1
pvalue=pchisq(chisq,df) ; pvalue
if (pvalue > 0.05){
print ("It is a poisson process")
} else {print ("It is not a poisson process")}
return(pvalue)
}
detect_poisson_process(y)
set.seed(1);y=rpois(100,5)
detect_poisson_process(y)
set.seed(1);y=rnorm(100)
detect_poisson_process(y)
?rnorm
set.seed(1);y=rnorm(100, 5, 0.2)
y
detect_poisson_process(y)
gf = goodfit(y,type= "poisson",method= "ML")
gf.summary = capture.output(summary(gf))[[5]]
pvalue = unlist(strsplit(gf.summary, split = " "))
pvalue = as.numeric(pvalue[length(pvalue)]); pvalue
source('D:/possion_process.R')
set.seed(1);y=rnorm(100)
gf = goodfit(y,type= "poisson",method= "ML")
gf.summary = capture.output(summary(gf))[[5]]
pvalue = unlist(strsplit(gf.summary, split = " "))
pvalue = as.numeric(pvalue[length(pvalue)]); pvalue
set.seed(1);y=rnorm(100)
gf.summary = capture.output(summary(gf))[[5]]
pvalue = unlist(strsplit(gf.summary, split = " "))
pvalue = as.numeric(pvalue[length(pvalue)]); pvalue
set.seed(2014);y=rnorm(100)
gf = goodfit(y,type= "poisson",method= "ML")
gf.summary = capture.output(summary(gf))[[5]]
pvalue = unlist(strsplit(gf.summary, split = " "))
pvalue = as.numeric(pvalue[length(pvalue)]); pvalue
set.seed(2014);y=rnorm(100, 5, 0.3)
gf = goodfit(y,type= "poisson",method= "ML")
gf.summary = capture.output(summary(gf))[[5]]
pvalue = unlist(strsplit(gf.summary, split = " "))
pvalue = as.numeric(pvalue[length(pvalue)]); pvalue
source('D:/possion_process.R')
source('D:/possion_process.R')
plot(gf,main="Count data vs Poisson distribution")
hist(y)
chisq = sum(  (gf$observed-gf$fitted)^2/gf$fitted )
df = length(gf$observed)-1-1
pvalue=pchisq(chisq,9) ; pvalue
source('D:/possion_process.R')
source('D:/possion_process.R')
source('D:/possion_process.R')
pvalue=pchisq(chisq,df) ; pvalue
set.seed(2014);y=rnorm(100, 5, 0.3) # goodfit asks for non-negative values
gf = goodfit(y,type= "poisson",method= "ML")
chisq = sum(  (gf$observed-gf$fitted)^2/gf$fitted )
df = length(gf$observed)-1-1
pvalue=pchisq(chisq,df) ; pvalue
set.seed(2014);y=rnorm(100, 5, 0.3) # goodfit asks for non-negative values
# to mannualy compute the pvalue
chisq = sum(  (gf$observed-gf$fitted)^2/gf$fitted )
df = length(gf$observed)-1-1
pvalue=pchisq(chisq,df) ; pvalue
set.seed(2014);y=rnorm(100, 5, 0.3) # goodfit asks for non-negative values
plot(gf,main="Count data vs Poisson distribution")
gf.summary = capture.output(summary(gf))[[5]]
pvalue = unlist(strsplit(gf.summary, split = " "))
pvalue = as.numeric(pvalue[length(pvalue)]); pvalue
chisq = sum(  (gf$observed-gf$fitted)^2/gf$fitted )
df = length(gf$observed)-1-1
pvalue=pchisq(chisq,df) ; pvalue
source('D:/possion_process.R')
pvalue=pchisq(chisq, df) ; pvalue
set.seed(2014);y=rnorm(100, 5, 0.3) # goodfit asks for non-negative values
chisq = sum(  (gf$observed-gf$fitted)^2/gf$fitted )
df = length(gf$observed)-1-1
pvalue=pchisq(chisq, df) ; pvalue
df
set.seed(2014);y=rnorm(100, 5, 0.3) # goodfit asks for non-negative values
gf = goodfit(y,type= "poisson",method= "ML")
gf
set.seed(2014);y=rpois(200,5)
gf = goodfit(y,type= "poisson",method= "ML")
plot(gf,main="Count data vs Poisson distribution")
chisq = sum(  (gf$observed-gf$fitted)^2/gf$fitted )
df = length(gf$observed)-1-1
pvalue=pchisq(chisq, df) ; pvalue
gf.summary = capture.output(summary(gf))[[5]]
pvalue = unlist(strsplit(gf.summary, split = " "))
pvalue = as.numeric(pvalue[length(pvalue)]); pvalue
source('D:/possion_process.R')
demo("cfplot_reg", "migest")
library(plyr)
library(stringr)
library(e1071)
as.list(c("a", "b"), c("d", "e"))
rbind(c("a", "b"), c("d", "e"))
pos_tweets =  rbind(
c('I love this car', 'positive'),
c('This view is amazing', 'positive'),
c('I feel great this morning', 'positive'),
c('I am so excited about the concert', 'positive'),
c('He is my best friend', 'positive'))
pos_tweets
pos_tweets =  rbind(
c('I love this car', 'positive'),
c('This view is amazing', 'positive'),
c('I feel great this morning', 'positive'),
c('I am so excited about the concert', 'positive'),
c('He is my best friend', 'positive')
)
neg_tweets = rbind(
c('I do not like this car', 'negative'),
c('This view is horrible', 'negative'),
c('I feel tired this morning', 'negative'),
c('I am not looking forward to the concert', 'negative'),
c('He is my enemy', 'negative')
)
pos_tweets
neg_tweets
class(pos_tweets)
test_tweets = rbind(
c('feel happy this morning', 'positive'),
c('larry friend', 'positive'),
c('not like that man', 'negative'),
c('house not great', 'negative'),
c('your song annoying', 'negative')
)
tweets = rbind(pos_tweets, neg_tweets)
tweets
twees[1,]
twees[,1]
tweets[,1]
get_word_list = function(sentence){
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
sentence = tolower(sentence)
wordList = str_split(sentence, '\\s+')
return(wordList)
}
lapply(tweets[,1], get_word_list)
wordlist = lapply(tweets[,1], get_word_list)
wordlsit[1]
wordlsit[[1]]
wordlist[1]
wordlist[[1]]
wordlist[[[1]]]
wordlist[[1]]
unique_words = unique(unlist(wordlist))
unique_words
word_features = unique(unlist(wordlist))
?naiveBayes
tweets = rbind(pos_tweets, neg_tweets, test_tweets)
matrix <- create_matrix(tweets, language="english",removeNumbers=TRUE, stemWords=FALSE, weighting=weightTfIdf)
library(RTextTools)
install.packages("RTextTools")
library(RTextTools)
matrix <- create_matrix(tweets[,1], language="english",removeNumbers=TRUE, stemWords=FALSE, weighting=weightTfIdf)
?create_matrix
matrix <- create_matrix(tweets[,1], language="english",removeNumbers=TRUE, stemWords=FALSE, weighting=weightTf)
library(RTextTools)
matrix <- create_matrix(tweets[,1], language="english",removeNumbers=TRUE, stemWords=FALSE, weighting=weightTf)
matrix <- create_matrix(tweets[,1], language="english",removeNumbers=TRUE, stemWords=FALSE)
matrix
as.matrix(matrix)
matrix <- create_matrix(tweets[,1], language="english",removeNumbers=TRUE,
stemWords=FALSE, tm::weightTfIdf)
print_algorithms()
models <- train_models(container, algorithms=c("MAXENT","SVM"
"GLMNET", "BOOSTING",
# "SLDA","BAGGING",
"RF", "NNET",
"TREE" ))
container <- create_container(matrix,data$Topic.Code,trainSize=1:10, testSize=11:15,virgin=TRUE) #可以设置removeSparseTerms
container <- create_container(matrix,tweets[,2],trainSize=1:10, testSize=11:15,virgin=TRUE) #可以设置removeSparseTerms
models <- train_models(container, algorithms=c("MAXENT","SVM"
"GLMNET", "BOOSTING",
# "SLDA","BAGGING",
"RF", "NNET",
"TREE" ))
models <- train_models(container, algorithms=c("MAXENT","SVM",
"GLMNET", "BOOSTING",
# "SLDA","BAGGING",
"RF", "NNET",
"TREE" ))
results <- classify_models(container, models)
dim(tweets)
tweets[,2]
container
models <- train_models(container, algorithms=c("MAXENT","SVM",
"GLMNET", "BOOSTING",
# "SLDA","BAGGING",
"RF", "NNET",
"TREE" ))
results <- classify_models(container, models)
classify_models
?classify_models
models <- train_models(container, algorithms=c("MAXENT","SVM",
"GLMNET", "BOOSTING",
# "SLDA","BAGGING",
"RF", #"NNET",
"TREE" ))
results <- classify_models(container, models)
analytics <- create_analytics(container, results)
results
analytics
analytics = create_analytics(container, results)
# 对以词语为特征向量的文本矩阵划分训练集和测试集
container = create_container(matrix, tweets[,2],
trainSize=1:10, testSize=11:15,virgin=TRUE) #可以设置removeSparseTerms
# 训练模型，可以选择其它方法
models = train_models(container, algorithms=c("MAXENT","SVM",
#"GLMNET", "BOOSTING",
# "SLDA","BAGGING",
"RF", #"NNET",
"TREE" ))
results = classify_models(container, models)
# 分析结果
analytics = create_analytics(container, results)
models = train_models(container, algorithms=c(#"MAXENT",
"SVM",
#"GLMNET", "BOOSTING",
# "SLDA","BAGGING",
"RF", #"NNET",
"TREE" ))
# 测试模型
results = classify_models(container, models)
# 分析结果
analytics = create_analytics(container, results)
# 训练模型，可以选择其它方法
models = train_models(container, algorithms=c(#"MAXENT",
#"SVM",
#"GLMNET", "BOOSTING",
# "SLDA","BAGGING",
"RF", #"NNET",
"TREE" ))
# 测试模型
results = classify_models(container, models)
# 分析结果
analytics = create_analytics(container, results)
results
tweets[,2]
library(plyr)
library(stringr)
library(e1071)
setwd("D:/Dropbox/sentimentCN/sentiment_analysis-master/")
##############
"read dict"
##############
#load up word polarity list and format it
afinn_list = read.delim(file='./AFINN/AFINN-111.txt', header=FALSE, stringsAsFactors=FALSE)
names(afinn_list) = c('word', 'score')
afinn_list$word = tolower(afinn_list$word)
#categorize words as very negative to very positive and add some movie-specific words
vNegTerms = afinn_list$word[afinn_list$score==-5 | afinn_list$score==-4]
negTerms = c(afinn_list$word[afinn_list$score==-3 | afinn_list$score==-2 | afinn_list$score==-1],
"second-rate", "moronic", "third-rate", "flawed", "juvenile", "boring", "distasteful",
"ordinary", "disgusting", "senseless", "static", "brutal", "confused", "disappointing",
"bloody", "silly", "tired", "predictable", "stupid", "uninteresting", "trite", "uneven",
"outdated", "dreadful", "bland")
posTerms = c(afinn_list$word[afinn_list$score==3 | afinn_list$score==2 | afinn_list$score==1],
"first-rate", "insightful", "clever", "charming", "comical", "charismatic",
"enjoyable", "absorbing", "sensitive", "intriguing", "powerful", "pleasant",
"surprising", "thought-provoking", "imaginative", "unpretentious")
vPosTerms = c(afinn_list$word[afinn_list$score==5 | afinn_list$score==4],
"uproarious", "riveting", "fascinating", "dazzling", "legendary")
#############################
"function of sentimentScore"
#############################
sentimentScore = function(sentences, vNegTerms, negTerms, posTerms, vPosTerms){
final_scores = matrix('', 0, 5)
scores = laply(sentences, function(sentence, vNegTerms, negTerms, posTerms, vPosTerms){
initial_sentence = sentence
#remove unnecessary characters and split up by word
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
sentence = tolower(sentence)
wordList = str_split(sentence, '\\s+')
words = unlist(wordList)
#build vector with matches between sentence and each category
vPosMatches = match(words, vPosTerms)
posMatches = match(words, posTerms)
vNegMatches = match(words, vNegTerms)
negMatches = match(words, negTerms)
#sum up number of words in each category
vPosMatches = sum(!is.na(vPosMatches))
posMatches = sum(!is.na(posMatches))
vNegMatches = sum(!is.na(vNegMatches))
negMatches = sum(!is.na(negMatches))
score = c(vNegMatches, negMatches, posMatches, vPosMatches)
#add row to scores table
newrow = c(initial_sentence, score)
final_scores = rbind(final_scores, newrow)
return(final_scores)
}, vNegTerms, negTerms, posTerms, vPosTerms)
return(scores)
}
################
"load data"
################
#load up positive and negative sentences and format
posText = read.delim(file='polarityData/rt-polaritydata/rt-polarity-pos.txt', header=FALSE, stringsAsFactors=FALSE)
negText = read.delim(file='polarityData/rt-polaritydata/rt-polarity-neg.txt', header=FALSE, stringsAsFactors=FALSE)
posText = posText$V1
negText = negText$V1
posText = unlist(lapply(posText, function(x) { str_split(x, "\n") }))
negText = unlist(lapply(negText, function(x) { str_split(x, "\n") }))
#build tables of positive and negative sentences with scores
posResult = as.data.frame(sentimentScore(posText, vNegTerms, negTerms, posTerms, vPosTerms))
negResult = as.data.frame(sentimentScore(negText, vNegTerms, negTerms, posTerms, vPosTerms))
posResult = cbind(posResult, 'positive')
negResult = cbind(negResult, 'negative')
colnames(posResult) = c('sentence', 'vNeg', 'neg', 'pos', 'vPos', 'sentiment')
colnames(negResult) = c('sentence', 'vNeg', 'neg', 'pos', 'vPos', 'sentiment')
#########################################################
"run the naive bayes algorithm using all four categories"
#########################################################
results = rbind(posResult, negResult)
classifier = naiveBayes(results[,2:5], results[,6])
predicted = predict(classifier, results)
results$predicted = predicted
results[1:50,2:7]
#display the confusion table for the classification ran on the same data
confTable = table(predicted, results[,6], dnn=list('predicted','actual'))
confTable
#run a binomial test for confidence interval of results
binom.test(confTable[1,1] + confTable[2,2], nrow(results), p=0.5)
matrix
matrix
as.matrix(container)
as.matrix(matrix)
mat = as.matrix(matrix)
dim(mat)
classifier = naiveBayes(mat, tweet[,2])
classifier = naiveBayes(mat, tweets[,2])
mat= create_matrix(tweets[,1], language="english",removeNumbers=TRUE,
stemWords=FALSE, tm::weightTfIdf)
mat = as.matrix(matrix)
classifier = naiveBayes(mat[1:10, ], tweets[1:10,2])
predict(classifier, mat[11:15,])
classifier = naiveBayes(mat[1:10, ], tweets[1:10,2])
predicted = predict(classifier, mat[11:15,])
predicted
classifier
mat[11:15,]
predicted = predict(classifier, mat[11:15,])
predict(classifier, mat[11:15,])
predict(classifier, mat)
classifier = naiveBayes(mat, tweets)
classifier = naiveBayes(mat, tweets[,2])
predict(classifier, mat)
predicted
classifier = naiveBayes(results[,2:5], results[,6])
predicted = predict(classifier, results)
predicted
